<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124600152-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-124600152-1');
    </script>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!--Load external scripts-->

    <script src="https://kit.fontawesome.com/9ffc1b7c87.js" crossorigin="anonymous"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script defer src="/scripts/prism.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!--Load local scripts-->
    <script defer src="/scripts/controller.js"></script>
    <script defer src="/scripts/random.js"></script>

    <!--Load styles-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
        integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="/styles/prism.css" />
    <link rel="stylesheet" href="/styles/main.css">
    <link rel="stylesheet" href="/styles/chapter.css">

    <title>RL: A/B Testing</title>
</head>

<body>

    <header>
        <!--Navbar to replace left-side menu on small screens-->
        <nav class="navbar navbar-expand-lg navbar-light bg-light">
            <a class="navbar-brand" href="#"></a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarText"
                aria-controls="navbarText" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarText">
                <ul class="navbar-nav mr-auto">
                    <li class="nav-item active">
                        <a class="nav-link" href="" data-page="/index.html"><i class="fas fa-home"></i>
                            Home
                            <span class="sr-only" id="nav-link-current">(current)</span></a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="" data-page="/blog/blog_index.html"><i class="fas fa-pencil-alt"></i>
                            Blog
                        </a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="" data-page="/tutorials/tutorial-index.html"
                            data-stylesheet="/styles/tutorial-index.css"><i class="fas fa-book"></i>
                            Tutorials </a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="" data-page='/teaching.html' data-stylesheet="/styles/teaching.css"><i
                                class="fas fa-chalkboard-teacher"></i>
                            Teaching</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="" data-page='/about.html'><i class="fas fa-info-circle"></i> About
                        </a>
                    </li>
                </ul>
            </div>
        </nav>

        <nav class='left-menu'>
            <img id="profile_img" src="https://github.com/Lourenzutti.png" alt="">
            <p id="name">Rodolfo Lourenzutti</p>
            <p id="title">Assistant Professor of Teaching @ UBC Department of Statistics </p>

            <hr class="menu_separator">

            <ul id="left-menu-items">
                <li>
                    <a class="menu_link" href="" data-page="/index.html"> <i class="fas fa-home"></i> Home </a>
                </li>
                <li>
                    <a class="menu_link" href="" data-page="/blog/blog_index.html"> <i class="fas fa-pencil-alt"></i>
                        Blog </a>
                </li>
                <li>
                    <a class="menu_link" href="" data-page="/tutorials/tutorial-index.html"
                        data-stylesheet="/styles/tutorial-index.css"> <i class="fas fa-book"></i>
                        Tutorials
                    </a>
                </li>
                <li>
                    <a class="menu_link" href="" data-page="/teaching.html" data-stylesheet="/styles/teaching.css"><i
                            class="fas fa-chalkboard-teacher"></i>
                        Teaching</a>
                </li>
                <li>
                    <a class="menu_link" href="" data-page="/about.html"><i class="fas fa-info-circle"></i> About </a>
                </li>
            </ul>

            <hr class="menu_separator">

            <div class="social">

                <div class="social_link">
                    <a class="social_link" href="https://github.com/Lourenzutti" target="_blank">
                        <i class="fab fa-github fa-2x"></i>
                    </a>
                </div>

                <div class="social_link" style="color: lightskyblue">
                    <a class="social_link" href="https://twitter.com/LourenzuttiR" target="_blank">
                        <i class="fab fa-twitter fa-2x"></i>
                    </a>
                </div>

                <div class="social_link" style="color: rgb(63, 63, 185);">
                    <a class="social_link" href="https://www.linkedin.com/in/rodolfo-lourenzutti/" target="_blank">
                        <i class="fab fa-linkedin fa-2x"></i>
                    </a>
                </div>

                <div class="social_link">
                    <a href="mailto:rodolfo.lourenzutti@gmail.com">
                        <i class="far fa-envelope fa-2x"></i>
                    </a>
                </div>
                <div class="social_link">
                    <a href="https://scholar.google.ca/citations?user=vT5b7lMAAAAJ&hl=en">
                        <i class="ai ai-google-scholar ai-2x"></i>
                    </a>
                </div>
            </div>
        </nav>
    </header>

    <main>
        <h1>A/B Testing</h1>

        <p>In STAT 201, we studied situations where the main interest is not the exact value of a population parameter
            but, instead, which of the two mutually exclusive possible scenarios, namely \(H_0\) or \(H_1\), is true.
            For example, we are not interested in the exact value of the population mean, \(\mu\), but we are interested
            in knowing if \(\mu\) is higher than \(35\)mm or not.</p>

        <p>In a classical hypothesis test, we generally use \(H_0\) as the status quo hypothesis (i.e., the hypothesis
            of
            nochange, no difference), where \(H_1\) represents the anticipated change. Note that \(H_1\) is the
            alternative hypothesis, in the sense that if \(H_0\) is false, then \(H_1\) is true. It is not allowed for
            both hypotheses to be false; one of the two hypotheses must be true. Also, the hypotheses are mutually
            exclusive (i.e., both hypotheses cannot be true simultaneously).</p>

        <p style="text-indent: 0;">The general procedure for hypothesis testing is always the same:</p>

        <ol>
            <li>
                <p>Define the hypotheses: \(H_0\) and \(H_1\); </p>
            </li>
            <li>
                <p>Specify the desired significance level.</p>
            </li>
            <li>
                <p>Define a test statistic, \(T\), appropriate to test the hypothesis.</p>
            </li>
            <li>
                <p>Study the distribution of the test statistic as if \(H_0\) were true. This distribution is called the
                    <em>null distribution</em>.
                </p>
            </li>
            <li>
                <p>Check the actual value of the test statistic using the data you collected.</p>
            </li>
            <li>
                <p>Contrast the value of the test statistic with the null distribution by calculating the
                    <em>p-value</em>.
                </p>
            </li>
            <li>
                <p>If the p-value is smaller than the significance level, reject \(H_0\), otherwise do not reject
                    \(H_0\).</p>
            </li>
        </ol>

        <h2>Motivational Problem</h2>
        <p>In 2008, Obama's campaign was looking to increase the total amount of donations to the campaign. To donate
            online, people needed to subscribe to the campaign e-mail by clicking on a red button saying "Sign Up".</p>

        <figure>
            <img src="/imgs/ab-testing/obama_homepage_original.png">
            <figcaption> Original website of the campaign.
                <p class="source-img">Source: <a
                        href="https://blog.optimizely.com/2010/11/29/how-obama-raised-60-million-by-running-a-simple-experiment/">Optimizely</a>
                </p>
            </figcaption>
        </figure>

        <p>After a while, they started wondering: Is this website's design effective? They decided to test it! The
            campaign created three alternative button designs (with text: <em>SIGN UP NOW</em>, <em>LEARN MORE</em>,
            <em>JOIN US NOW</em>) and five different media choices to replace the original photo: two alternative photos
            and three videos. In total, they compared 24 website designs.
            they measured the subscription rate of each design. As it turns out, the best design had over 40% higher
            subscription rate than the original website being used. It was estimated that the additional subscription
            generated an additional 60 million dollars worth of donations and 288,000 additional volunteers. You can
            learn more about this application of A/B testing <a
                href="https://www.optimizely.com/insights/blog/how-obama-raised-60-million-by-running-a-simple-experiment/">here</a>.
        </p>

        <p>But how do we compare websites?</p>

        <h5>The response variable</h5>
        <p>The first step is to understand the purpose of the website. This is a fundamental
            step because it guides the creation of useful metrics of success. Defining the main
            purpose of a website is not always a simple task. For example, in their case:</p>
        <ul>
            <li>
                <p>Do they want the website to attract more subscribers?</p>
            </li>
            <li>
                <p>Do they want a high proportion of visitors to become donors?</p>
            </li>
            <li>
                <p>Do they want to increase the size of donation per visitor?</p>
            </li>
        </ul>
        <p>What is a good metric to measure how effective the website is? Such metric will be
            the <strong>response variable</strong> of the study. Although the campaign wanted
            to increase the total amount of donations, this was not the website's purpose.
            The purpose of the website was to attract subscribers. For this reason, they used
            the rate of subscription, i.e., the number of people that subscribed divided by the
            number of people that visited the website. If the website's purpose is not very well
            defined, you might (and probably will) come up with metrics that are misleading on how
            effective your website is. </p>

        <h5>The covariates</h5>
        <p>The second step is to identify the elements that could be optimized. In their case,
            they considered the media and the button. But they could consider other factors too,
            such as the background colour, for example. In essence, they are trying to find the
            configuration of the covariates, media and button, that would yield the highest
            subscriber rate.</p>

        <h5>Randomization</h5>
        <p>To avoid bias, each visitor saw a randomly chosen website design. This is a key step
            to be able to conclude that the reason for the increase in the subscription was the
            website's design, and not a hidden factor that is not even being considered,
            <em>lurking variable</em>. The idea is that randomization will "average out" all these
            hidden differences between the visitors, and the only difference between the groups would
            be the website seen.
        </p>


        <h2>A/B Testing</h2>

        <p> A/B testing is not only about website optimization. In general, we have two populations (or groups),
            namely Group A and Group B (hence A/B), and we want to compare these two populations with respect
            to a variable of interest (response variable). Let's see a few examples:</p>
        <p><span class="example"></span>A new vaccine has been developed for cancer. The drug company wants to check the
            efficacy of the vaccine. The company randomly split 50,000 volunteers into two groups, where 25,000 will
            receive the vaccine (Group A) and 25,000 will receive the placebo (Group B). They measure if the individuals
            develop cancer in the next ten years. </p>
        <ul>
            <li>
                <p><u><em>Response variable</em></u> (\(Y\)): whether the individuals develop cancer;
            </li>
            </p>
            <li>
                <p><u><em>Covariate</em></u> (\(X\)): whether the individuals receives the vaccine or the placebo
                    (two-levels);
            </li>
            </p>
            <li>
                <p><u><em>Parameters of interest</em></u>: \(p_1\) and \(p_2\), the proportions of individuals who
                    develop cancer
                    in Group A and Group B, respectively;</p>
            </li>
            <li>
                <p><u><em>Research question</em></u>: is the vaccine effective? In other words, is \(p_1 < p_2\)?</p>
            </li>
        </ul>
        <div class="end-part"></div>

        <p><span class="example"></span>
            A phone company wants to reduce the number of complaints against its customer services. They are considering
            removing
            the navigation menu from the support service and using support staff instead. Naturally, this will be an
            expensive move,
            so they first want to test it to see if it would be effective. They trained a small team and randomly
            directed the clients
            to the navigation menu or human support. Then, they monitor whether the clients will open a complaint at the
            Canadian
            Radio-television and Telecommunications Commission (CRTC).</p>
        <ul>
            <li>
                <p><u><em>Response variable</em></u> (\(Y\)): whether the individual opens a complaint;</p>
            </li>
            <li>
                <p><u><em>Covariate</em></u> (\(X\)): whether the individual is directed to the navigation menu or the
                    support staff (two-levels);</p>
            </li>
            <li>
                <p><u><em>Parameters of interest</em></u>: \(p_1\) and \(p_2\), the proportion of clients that open a
                    complaint at CRTC in Group A and Group B, respectively;</p>
            </li>
            <li>
                <p><u><em>Research question</em></u>: is the support staff better? In other words, is \(p_1 < p_2\)?</p>
            </li>
        </ul>
        <div class="end-part"></div>

        <p><span class="example"></span> An e-commerce company wants to compare two website designs with respect to
            sales
            in dollars. For the following \(N\) clients, the design each client will see will be selected at random.</p>
        <ul>
            <li>
                <p><u><em>Response variable</u></em> (\(Y\)): the amount of dollars spent;</p>
            </li>
            <li>
                <p><u><em>Covariate (\(X\))</u></em>: two website designs (two-levels);</p>
            </li>
            <li>
                <p><u><em>Parameters of interest</u></em>: \(\mu_1\) and \(\mu_2\), the average amount of dollars spent
                    by the clients in each website design;</p>
            </li>
            <li>
                <p><u><em>Research question</u></em>: Is one of the designs better? </p>
            </li>
        </ul>
        <div class="end-part"></div>

        <p>In general, the structure of A/B Testing consists of: (1) a response variable, \(Y\); (2) a covariate, \(X\),
            that splits the population into two groups; (3) randomization, individuals are randomly assigned to the
            groups; and (4) statistical comparison of the groups' parameter of interest (remember that all we have is a
            sample, so we need to account for the sampling variability. </p>

        <figure>
            <img src="/imgs/ab-testing/ab-diagram.png" width="75%">
            <figcaption> A/B Testing Workflow</figcaption>
        </figure>

        <h4>How is Obama's Campaign Problem Different?</h4>

        <p>Note that in case of the Obama's campaign website, we have:</p>
        <ol>
            <li>
                <p>Response variable: the subscription rate; &#x2705;;</p>
            </li>
            <li>
                <p>Randomization; &#x2705;;</p>
            </li>
            <li>
                <p>One covariate that splits the population into two groups; <span style="color: red;">&#x2718;</span>
                </p>
            </li>
        </ol>
        <p> We had <strong>two</strong> covariates, \(X_1\) and \(X_2\), which are the button design and the media used,
            respectively. Besides, \(X_1\) had four levels (i.e., possible values): the four design options or the
            button. This would split the population into four groups, not into two. \(X_2\), the media variable, had six
            levels, three photos and three videos. Combining \(X_1\) and \(X_2\) results in a total of 24 groups.</p>

        <p>This problem is slightly more complex than having only two groups. If we performed pairwise comparisons, we
            would need 276 tests to compare all 24 groups. As we learned previously, this would considerably inflate the
            probability of error. For now, we will restrict our focus to only two groups.</p>

        <h3>Comparing the two groups</h3>
        <p>Once the appropriate response variable and covariate have been specified, we start collecting the data. The
            data collected will be only a sample of the population; therefore, we need to take into account the sampling
            variability. We are already familiar with the methodology to conduct this statistical analysis, namely
            two-samples
            confidence intervals and hypothesis tests. Let's refresh our memory!</p>

        <p>When estimating or testing hypotheses, the parameter of interest affects which statistic we are going to
            use. For example, when testing the mean, we want to use the sample mean \(\bar{X}\), when testing difference
            in proportion, we want to use the difference in sample proportions, \(\hat{p}_1 - \hat{p}_2\), and so
            on.</p>

        <p>Naturally, the way these statistics behave are different, i.e., the sampling distributions (and null models)
            of these statistics are different. We have explored two main approaches to approximate the
            sampling distribution (for confidence intervals) and the null model (for hypothesis testing): (1) the
            Central Limit Theorem (CLT); and (2) Simulation Based Approaches.</p>

        <h4>Central Limit Theorem</h4>
        <p> When estimating or testing hypotheses, the parameter of interest affects which statistic we are going to
            use. For example, when testing the mean, we want to use \(\bar{X}\), whereas when testing the pro

            The Central Limit Theorem provides an approximation for the distribution of certain test statistics for
            large sample size.

        </p>

        <h5>Comparing two means</h5>
        <p>Suppose you want to test the difference between two <em>independent</em> populations' means. The scenarios to
            be considered:</p>
        <ul>
            <li>
                <p>\(H_0: \mu_A - \mu_B = d_0\) vs \(H_1: \mu_A - \mu_B \neq d_0\)</p>
            </li>
            <li>
                <p>\(H_0: \mu_A - \mu_B = d_0\) vs \(H_1: \mu_A - \mu_B > d_0\)</p>
            </li>
            <li>
                <p>\(H_0: \mu_A - \mu_B = d_0\) vs \(H_1: \mu_A - \mu_B < d_0\)</p>
            </li>
        </ul>
        <p>To conduct this hypothesis test, we take two independent samples, one from each population. By independent
            samples, we meant that the individuals are selected independently from each population.</p>

        <p>Suppose Group A has \(n_A\) elements drawn at random from Population A, and Group B has \(n_B\) elements
            drawn at random from Population B. Let's use \(x\) to refer to Group A and \(y\) to refer to Group B. For
            large samples sizes, the test statistic given by </p>
        $$
        T = \frac{\bar{x}-\bar{y} - d_0}{\sqrt{\frac{s_A^2}{n_A} - \frac{s_B^2}{n_B}} }
        $$
        follows a \(t\)-distribution with approximately \(\nu\) degrees of freedom under \(H_0\), where
        $$
        \nu = \frac{
        \left(\frac{s_A^2}{n_A}+\frac{s_B^2}{n_B}\right)^2
        }
        {
        \frac{s_A^4}{n_A^2(n_A-1)}+\frac{s_2^4}{n_B^2(n_B-1)}
        }.
        $$

        <p style="text-indent: 0;">In other words, the <em>null model</em> of the test statistic above is \(t_\nu\). Of
            course, we are never going to calculate this weird formula by hand! Our computers
            can do this for us.</p>

        <p><span class="example"></span>
            Suppose Obama's campaign wanted to test which of two websites, <em>Website A</em> or
            <em>Website B</em>, results in a larger amount of donations. The next 60 users who visit the campaign's
            website will access one of the websites chosen at random until 30 users have seen one of the designs. We
            have collected (actually simulated!) this data for you and stored it in the
            <code>sample_money_donated</code> object.
        </p>

        <pre>
<code class="language-r">library(tidyverse)
set.seed(1)

# Simulating a sample of 30 individuals for group A and Group B
sample_money_donated <- 
    tibble(group = c("Website A", "Website B"), 
            amount = list(
                if_else(runif(30) < 0.5, 0, rnorm(30, 80, 10) ), 
                if_else(runif(30) < 0.6, 0, rnorm(30, 100, 20) ))
            ) %>% 
    unnest(cols = amount) %>%
    sample_n(60)

sample_money_donated</code></pre>
        <pre class="output">    <code>## # A tibble: 60 x 2
    ##    group     amount
    ##    <chr>      <dbl>
    ##  1 Website B    0  
    ##  2 Website B    0  
    ##  3 Website B    0  
    ##  4 Website A    0  
    ##  5 Website A   87.8
    ##  6 Website B    0  
    ##  7 Website A   66.2
    ##  8 Website A    0  
    ##  9 Website B   88.6
    ## 10 Website A    0  
    ## # ... with 50 more rows 
</code></pre>
        <p>Next, to test \(H_0: \mu_A - \mu_B = d_0\) vs \(H_1: \mu_A - \mu_B \neq d_0\), we can use the
            <code><a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/t.test" target="_blank">t.test</a></code>
            function in R.
        </p>
        <pre> <code class="language-r">t.test(amount ~ group, # The formula: "website affects amount".
        mu = 0, # the value of d0
        alternative = "two.sided", # "less" for &lt; and "greater" for &gt;
        data = sample_money_donated)</code>
        </pre>
        <pre class="output">    <code>##  Welch Two Sample t-test
    ## 
    ## data:  amount by group
    ## t = -0.80171, df = 55.245, p-value = 0.4262
    ## alternative hypothesis: true difference in means is not equal to 0
    ## 95 percent confidence interval:
    ##  -33.35773  14.29329
    ## sample estimates:
    ## mean in group Website A mean in group Website B 
    ##                37.28221                46.81443</code></pre>
        <div class="end-part"></div>

        <h5>Comparing two proportions</h5>

        <p>Obama's campaign wanted the website to increase the number of subscribers and, for this reason, they used the
            rate of subscription. In this case, the variable of interest, <em>"whether a visitor subscribes"</em>, is
            not numerical, is dichotomic: "yes" or "no". Consequently, we would want to compare the proportions of
            visitors who subscribes using <em>Website A</em> and <em>Website B</em>.</p>

        <p>To test for the equality of proportions between two groups, i.e., \(H_0: p_A - p_B = 0\) vs \(H_1: p_A - p_B
            \neq 0\),
            we can use the following test statistics:</p>
        $$
        Z = \frac{\hat{p}_A-\hat{p}_B}{\sqrt{\hat{p}(1-\hat{p})\left(\frac{1}{n_A}+\frac{1}{n_B}\right)}},
        $$
        <p style="text-indent: 0;">where \(\hat{p}=\frac{n_A\hat{p}_A+n_B\hat{p}_B}{n_A+n_B}\) is the overall
            proportion. For large sample sizes, the null
            model of the \(Z\) statistic is the Standard Normal distribution, \(N(0,1)\). Again, we will not do this
            manually, as we can
            easily calculate using the computer. </p>

        <p><span class="example"></span>
            Suppose Obama's campaign wanted to test which of two websites, <em>Website A</em> or
            <em>Website B</em>, results in a higher rate of subscribers. The next 60 users who visit the campaign's
            website will access one of the websites chosen at random until 30 users have seen one of the designs. We
            have collected this data for you and stored it in the
            <code>sample_subscriber</code> object.
        </p>
        <pre>
<code class="language-r">library(tidyverse)
set.seed(1)

# Simulating a sample of 30 individuals for group A and Group B
sample_subscriber <- 
    tibble(website = factor(c("A", "B")), 
            subscribed = list(
                sample(factor(c("yes", "no")), 30, replace = TRUE, p=c(0.42, 0.58)), 
                sample(factor(c("yes", "no")), 30, replace = TRUE, p=c(0.37, 0.58)))
            ) %>% 
    unnest(cols = subscribed) %>%
    sample_n(60)

sample_subscriber</code></pre>
        <pre class="output">    <code>## # A tibble: 60 x 2
    ##    website subscribed
    ##    &lt;fct&gt;   &lt;fct&gt;     
    ##  1 B       no        
    ##  2 B       yes       
    ##  3 A       yes       
    ##  4 A       no        
    ##  5 B       no        
    ##  6 A       no        
    ##  7 A       no        
    ##  8 B       no        
    ##  9 A       yes       
    ## 10 A       no        
    ## # ... with 50 more rows</code></pre>

        <p>Once the data is collected, we can run
            <code><a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prop.test" target="_blank">prop.test</a></code>
            in R to test the hypothesis.
        </p>
        <pre> <code class="language-r">prop.test(
    sample_subscriber %>% 
    group_by(website) %>% 
    summarise(n_successes = sum(subscribed == "yes"), 
              n_failures = sum(subscribed == "no")) %>% 
    select(-website) %>% 
    as.matrix())</code></pre>
        <pre class="output">    <code>##  2-sample test for equality of proportions with continuity correction
    ## 
    ## X-squared = 1.4459e-30, df = 1, p-value = 1
    ## alternative hypothesis: two.sided
    ## 95 percent confidence interval:
    ##  -0.2493486  0.3160153
    ## sample estimates:
    ##    prop 1    prop 2 
    ## 0.4333333 0.4000000</code></pre>
        <div class="end-part"></div>
        <h4>Simulation Based Approach</h4>
        <p>Alternatively to the CLT, we could use computer simulations to compare the groups. The same idea we discussed
            previously:</p>

        <ol>
            <li>
                <p>Collect the samples of Group A and Group B;</p>
            </li>
            <li>
                <p>Calculate the observed test statistic; </p>
            </li>
            <li>
                <p>Put the sample of both groups together;</p>
            </li>
            <li>
                <p>Select \(n_1\) elements at random, <strong>without</strong> replacement, from the two samples
                    combined.</p>
            </li>
            <li>
                <p>The selected elements will be your "new" Group A, and the remaining will be your "new" Group B. </p>
            </li>
            <li>
                <p>Calculate the appropriate test statistic (e.g., the difference of sample proportions or of sample
                    means).</p>
            </li>
            <li>
                <p>Perform Steps 4, 5 and 6, multiple times to obtain a sample from the <em>Null Distribution </em>.
                </p>
            </li>
            <li>
                <p>Compare the observed test statistic with the simulated null model.</p>
            </li>
        </ol>

        <p>Note that by randomizing the groups in the sample (Steps 3, 4, and 5), we are trying to remove the difference
            between the groups, except for those differences due to sampling variability. But why are we doing that
            again? Because we want to simulate a sample from the Null Model, which is the model under the assumption
            that there is no difference between the groups. </p>

        <p><span class="example"></span>
            Let's return to Obama's campaign problem in the example above. We want to test which website, Website A or
            Website B, has a higher subscription rate. The data is stored in <code>sample_subscriber</code>.
        </p>
        <pre>
<code class="language-r">sample_subscriber</code></pre>
        <pre class="output">    <code>## # A tibble: 60 x 2
    ##    website subscribed
    ##    &lt;fct&gt;   &lt;fct&gt;     
    ##  1 B       no        
    ##  2 B       yes       
    ##  3 A       yes       
    ##  4 A       no        
    ##  5 B       no        
    ##  6 A       no        
    ##  7 A       no        
    ##  8 B       no        
    ##  9 A       yes       
    ## 10 A       no        
    ## # ... with 50 more rows</code></pre>
        <p>First, we calculate the observed test statistic. Since we are comparing the proportions, we can use the
            difference of sample proportions as our test statistic.</p>
        <pre>
<code class="language-r">obs_test_statistic <- 
    sample_subscriber %>% 
    specify(formula = subscribed ~ website, success = "yes") %>% 
    calculate(stat = "diff in props", order = c("A", "B"))
  
obs_test_statistic</code></pre>
        <pre class="output">    <code>## # A tibble: 1 x 1
    ##     stat
    ##    &lt;dbl&gt;
    ## 1 0.0333</code></pre>
        <p>Next, we simulate a sample from the null model by following Steps 3-6. Luckily for us, we don't need to do
            this
            manually as we are already familiar with <code>infer</code>'s workflow.</p>

        <pre> <code class="language-r">sample_null_model <- 
    sample_subscriber %>% 
    specify(formula = subscribed ~ website, success = "yes") %>% 
    hypothesise(null = "independence") %>% 
    generate(reps = 5000, type = "permute") %>% 
    calculate(stat = "diff in props", order = c("A", "B"))
  
  sample_null_model</code></pre>
        <pre class="output">    <code>## # A tibble: 5,000 x 2
    ##    replicate    stat
    ##        &lt;int&gt;   &lt;dbl&gt;
    ##  1         1  0.1   
    ##  2         2 -0.1   
    ##  3         3 -0.0333
    ##  4         4 -0.233 
    ##  5         5 -0.0333
    ##  6         6 -0.0333
    ##  7         7  0.0333
    ##  8         8  0.167 
    ##  9         9 -0.1   
    ## 10        10  0.233 
    ## # ... with 4,990 more rows</code></pre>

        <p>Finally, we compare the observed test statistic with the null model. Since the observed test statistic is
            0.0333, we calculate the proportion of values generated from the null model that are above 0.0333 or below
            -0.0333. This will be the estimated p-value of the test. </p>

        <pre> <code class="language-r">sample_null_model %>% 
    get_p_value(obs_test_statistic, direction = "both")</code></pre>
        <pre class="output">    <code>## # A tibble: 1 x 1
    ##   p_value
    ##     <dbl>
    ## 1   0.998</code></pre>
        <div class="end-part"></div>

        <p>There's nothing new in these analyses, right? We have already discussed statistical inference
            involving two-sample. So, why are we discussing this again? As it turns out, A/B Testing is just another
            name that was coined in the industry for Randomized Experiments, a well-established statistical methodology
            for experimentation. Hence the familiarity of the methods above. That would be the end of it, except that
            some challenges arise in practice that requires close attention. </p>

        <h3>Early Stopping</h3>

        <p>In many situations, there is crushing evidence that a group is performing better than the other. So, should a
            company keep spending resources and time to continue the experiment as planned? Continuing with the
            experiment could be unnecessarily costly.</p>

        <p style="margin-bottom: 0;"><span class="example"></span>Suppose Pfizer, a pharmaceutical company, is
            conducting a clinical trial to test
            the effectiveness of a new treatment. They planned to have 1,000 participants in total, 500 of which will
            receive the new drug, and the remaining 500 will receive the placebo. However, at the current point in time,
            they have data of 600 participants, where 300 received the drug and 300 received the placebo. Among the 300
            who received the drug, nobody died. On the other hand, among the 300 who received the placebo, 200 died.
            Should the FDA (Food and Drug Administration) still wait for the result of the remaining 400 participants?
            Or should they stop the clinical trial early and start distributing the medicine to people in need?</p>
        <div class="end-part"></div>

        <!--Make the following two paragraphs an exercise-->
        <p>In general, we not only want to compare Groups A and B, but we also want to reach a conclusion as soon as
            possible. To do so, one would have to "peek" at the partially collected data to conduct the proper
            statistical analysis. However, when should we peek at the partially collected data?
        </p>

        <p>A reasonable answer could be to monitor the data online. In other words, for every new observation in each
            group we have, we take a peek at the data. </p>

        <p><span class="example"></span>Suppose Obama's campaign wanted to test two website designs, and they are
            interested in the amount of money donated per visitor.

            So, they decided to conduct an A/B Test and start monitoring as soon as they had ten visitors for each
            design. Then, they would peek at the data again once both designs had received another visitor, and so on.
            If they detect a difference, they will immediately stop the experiment and make the best website version
            available to all visitors. They will allow a maximum number of visitors of 1,000 per design.</p>

        <figure>
            <img src="/imgs/ab-testing/obama-experiment-1.png" width="50%">
            <figcaption> A/B Test with two versions of Obama's campaign website.
                <p class="source-img">Source: Obama's pictures<a
                        href="https://blog.optimizely.com/2010/11/29/how-obama-raised-60-million-by-running-a-simple-experiment/">Optimizely</a>
                </p>
            </figcaption>
            <figure>
                <img src="/imgs/ab-testing/obama-online-pvalues.png" width="60%">
                <figcaption> Multiple checks of p-value for different sample sizes
                    <p class="source-img">Source: Obama's pictures <a
                            href="https://blog.optimizely.com/2010/11/29/how-obama-raised-60-million-by-running-a-simple-experiment/">Optimizely</a>
                    </p>
                </figcaption>
            </figure>
        </figure>

        <div class="end-part"></div>
        <p>In the example above, Obama's campaign wants to use a strategy that does not specify the sample size. So far,
            we have been discussing inference scenarios where the sample size is determined <strong>prior to the
                study</strong>. But in this case, the methodology will not change because the sample size will be fixed
            at each <em>peek</em>. The problem is that we are experimenting multiple times with different sample sizes.
        </p>

        <div class="box-def box-exercise">
            <p></p>
            <p>
                When conducting hypothesis testing, what are the effects that sample size has on:
            </p>
            <ol>
                <li class="question-item">
                    <p>Probability of Type I Error</p>
                    <textarea class="answer" style="height:50px;"></textarea><br>
                </li>
                <li class="question-item">
                    <p>Probability of Type II Error</p>
                    <textarea class="answer" style="height:50px;"></textarea><br>
                </li>
                <li class="question-item">
                    <p>Power of the test</p>
                    <textarea class="answer" style="height:50px;"></textarea>
                </li>
            </ol>
            <button class="btn-show-answers">Show answers</button>
            <div class="solution">
                <dl>
                    <dt>Probability of Type I Error</dt>
                    <dd>None. Remember that the probability of Type I Error is the significance level, which is
                        specified by us before the test is conducted.</dd>

                    <dt>Probability of Type II Error</dt>
                    <dd>The Probability of Type II Error decreases as the sample size increases. The reason is that
                        both the null model and the sampling distribution of the test statistic will become narrower,
                        hence reducing their overlap.
                    </dd>

                    <dt>Power of the test</dt>
                    <dd> The power of the test is just one minus the probability of Type II Error. Since the probability
                        of Type II Error decreases as the sample size increases, the power of the test increases as the
                        sample size increases.
                    </dd>
                </dl>
            </div>
        </div>


        <p>Ok, so Obama's campaign seems to have a good plan. The probability of Type I Error at each peek will be the
            same; the power of the test will be smaller, but if it is already big enough to detect a difference, kudos!
            No downside here, right? Wrong!While the probability of Type I Error is the same at each peek, they only
            need to reject the hypothesis once in multiple tests.
        </p>

        <div class="box-def box-exercise">
            <p></p>
            <p style="text-indent: 0;">
                When conducting multiple hypothesis testing, what happens to the family-wise errors?
            </p>
            <textarea class="answer" style="height:50px; width: 90%;"></textarea><br>
            <button class="btn-show-answers">Show answers</button>
            <div class="solution">
                Since you need to make the right decision multiple times, and each time there is a chance that you make
                the wrong decision,
                when you consider whether <strong>all</strong> the decisions you made are right, there is a much lower
                chance of that happening compared to each hypothesis testing individually.
            </div>
        </div>

        <p><span class="example"></span> Let's continue our previous example of Obama's campaign. The first thing we
            should do is to test if the plan is good. But how can we do that if we do not know the truth (i.e., if
            there's actually a difference between the two websites): Here's our we are going to do it:
        <ol>
            <li>
                <p>Let's run the experiment 100 times.</p>
                <figure>
                    <img src="/imgs/ab-testing/obama-experiment-100.png" width="75%">
                    <figcaption> One hundred A/B Tests with two versions of Obama's campaign website.
                        <p class="source-img">Source: Obama's pictures <a
                                href="https://blog.optimizely.com/2010/11/29/how-obama-raised-60-million-by-running-a-simple-experiment/">Optimizely</a>
                        </p>
                    </figcaption>
                </figure>


            </li>
            <li>
                <p>But this time, we are going to use the same website for both groups. This way, we know the truth --
                    there's
                    no difference!!</p>
                <figure>
                    <img src="/imgs/ab-testing/obama-aa-experiment-100.png" width="75%">
                    <figcaption> One hundred A/B Test with the same version of Obama's campaign website.
                        <p class="source-img">Source: Obama's pictures <a
                                href="https://blog.optimizely.com/2010/11/29/how-obama-raised-60-million-by-running-a-simple-experiment/">Optimizely</a>
                        </p>
                    </figcaption>
                </figure>
                <figure>
                    <img src="/imgs/ab-testing/obama-online-monitoring-pvalue-100.png" width="60%">
                    <figcaption> Multiple checks of p-value for different sample sizes in each of the 100 experiments.
                        <p class="source-img">Source: Obama's pictures <a
                                href="https://blog.optimizely.com/2010/11/29/how-obama-raised-60-million-by-running-a-simple-experiment/">Optimizely</a>
                        </p>
                    </figcaption>
                </figure>
            </li>
            <li>
                <p>Let's see in how many of the 100 experiments the null hypothesis is rejected at least once. That will
                    estimate how bad the chance of our <em>family-wise</em> Type I Error will be.</p>
            </li>
        </ol>
        </p>
        <p>In this case, using a significance level of 5%, approximately 22% of the 100 A/B tests would have wrongly
            rejected \(H_0\) and concluded that the same version of the website is different from itself. This is way
            above
            our specified significance level of 5%. Terrible, isn't it? Obama's campaign is in a bit of a
            pickle now. It seems they have to wait for 2000 people to visit the website before they can examine whether
            there's a difference. Do they, though? </p>
        <p>We have learned about p-value adjustments to control for Type I Error. Remember Bonferroni's correction?
            So we could propose a compromise to
            them:<br>
            &emsp;&emsp;<em>- "Hey, don't peek at the data for every two people that visit your website.
                Instead, pick a reasonable number of tests, and I'll adjust the p-value so we can control the
                probability of Type I Error."</em></p>

        <p style="text-indent: 0;">They can decide when to conduct the test, say for example, n = 50, n = 100, n = 250,
            n = 500, and n = 1000. Doing this in the Obama's problem would reduce the number of rejection to 7
            experiment, much closer to the 5% specified. This approach is called <em>principled peeking</em>.</p>









        <div style="height: 400px;">

        </div>

    </main>

</body>

</html>